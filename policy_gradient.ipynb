{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7460a5",
   "metadata": {},
   "source": [
    "We will be researching the Cart-pole problem with a policy gradient agorithim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46013b",
   "metadata": {},
   "source": [
    "We Must first build our enviorment that are reinforcement learning model will learn in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7a8b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco\n",
    "import mujoco_viewer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e23484",
   "metadata": {},
   "source": [
    "Now let's create the neccesary functions for our enviorment\n",
    "\n",
    "Our model will take in 4 values: X value, X velocity, Theta value, Theta velocity.\n",
    "\n",
    "We will need the enviorment randomized every new episode played by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a54cb5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rand\n",
    "import math\n",
    "def Intial_state():\n",
    "    theta = rand.uniform(-math.radians(6), math.radians(6))\n",
    "    xpos = rand.uniform(-0.05, 0.05)\n",
    "    xdot = rand.uniform(-0.5, 0.5)\n",
    "    theta_dot = 0\n",
    "    return xpos, xdot, theta, theta_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b5274",
   "metadata": {},
   "source": [
    "Now lets make the neccessary functions for our model. It needs to pushed to the left and to the right. We will use the data.crtl[0] to use the simulated acctuators that are in our model. The data element is all of the data of the cartpole at any moment this includes X Velo, theta Velo, X Pos, Theta Pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "722a33f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "FORCE_MAG = 2.0\n",
    "\n",
    "def right(data, force=FORCE_MAG):\n",
    "    data.ctrl[0] = np.clip(force, -2.0, 2.0)\n",
    "\n",
    "\n",
    "def left(data, force=FORCE_MAG):\n",
    "    data.ctrl[0] = np.clip(-force, -2.0, 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2953de",
   "metadata": {},
   "source": [
    "Our model will need to know the state it is in, so we will create a function to get the raw values of the state for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99df24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_raw_state(data):\n",
    "    xpos = data.qpos[0]\n",
    "    xdot = data.qvel[0]\n",
    "    theta = data.qpos[1]\n",
    "    theta_dot = data.qvel[1]\n",
    "    return np.array([xpos, xdot, theta, theta_dot], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f1241",
   "metadata": {},
   "source": [
    "We also need a function to normalize the state to make it easeier for the model to make outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c39f1564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_state(raw_state):\n",
    "    x, x_dot, theta, theta_dot = raw_state\n",
    "    sin_theta = math.sin(theta)\n",
    "    cos_theta = math.cos(theta)\n",
    "    norm_x = np.clip(x / 1.0, -1.0, 1.0)\n",
    "    norm_x_dot = np.tanh(x_dot)\n",
    "    norm_theta_dot = np.tanh(theta_dot)\n",
    "    return np.array([norm_x, norm_x_dot, sin_theta, cos_theta, norm_theta_dot], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee6268",
   "metadata": {},
   "source": [
    "Now we will create the reward funcition for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02579409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limits to determine if the episode is done\n",
    "THETA_LIMIT_RADIANS = math.radians(12)\n",
    "X_LIMIT = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e0fd652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(raw_state):\n",
    "    x, x_dot, theta, theta_dot = raw_state\n",
    "    angle_error = abs(theta)\n",
    "\n",
    "    position_error = abs(x)\n",
    "    reward = 1.0\n",
    "    reward -= 2.0 * (angle_error / THETA_LIMIT_RADIANS)\n",
    "    reward -= 0.5 * (position_error / X_LIMIT)\n",
    "    reward -= 0.01 * (abs(x_dot) + abs(theta_dot))\n",
    "    return max(reward, -2.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d087c1",
   "metadata": {},
   "source": [
    "Now we will make a function to check if an episode is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a10ab3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_finished(raw_state):\n",
    "    x, _, theta, _ = raw_state\n",
    "    return abs(theta) > THETA_LIMIT_RADIANS or abs(x) > X_LIMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934afc0a",
   "metadata": {},
   "source": [
    "Now we must create our Nueral Network for the cartpole problem\n",
    "We will be creating a nueral netowrk through pytorch. It will take in 4 dimensions as an input for the network: X Value, X Velocity, Theta value, Theta Velocity\n",
    "\n",
    "We will have 3 layers in the netowrk, the first will take in the state layer, second being an input from the frist, and the thrid being an input from the second and outputting an action from our action set. Our action set is either sliding to the left or sliding to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e60ebeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self,state_dim = 5, action_dim = 2, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return self.output_layer(x)  # logits (unnormalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f717110",
   "metadata": {},
   "source": [
    "We will also make a sample action function for the nueral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2441426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(policy_net, state):\n",
    "    state_t = torch.as_tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    logits = policy_net(state_t)  # shape [1, action_dim]\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "    action = dist.sample()  # shape [1]\n",
    "    log_prob = dist.log_prob(action)  # shape [1]\n",
    "    entropy = dist.entropy()  # shape [1] - PROPER entropy over all actions\n",
    "    return action.item(), log_prob.squeeze(0), entropy.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62eb961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discounted_returns(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute discounted returns WITHOUT normalization.\n",
    "    Normalization within an episode destroys the learning signal!\n",
    "    \"\"\"\n",
    "    discounted_returns = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        discounted_returns.insert(0, R)\n",
    "    return torch.FloatTensor(discounted_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cc3e24",
   "metadata": {},
   "source": [
    "Code to visualize total reward over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "240c66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(total_rewards_over_time):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(total_rewards_over_time, label='total_reward')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Total Reward Over Time')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('total_rewards_over_time_plot.png', dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fbe5c1",
   "metadata": {},
   "source": [
    "Now lets add our necessary constants for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "687f9f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_TRAINING = False\n",
    "MODEL_PATH = 'cartpole.xml'\n",
    "EPISODES = 300\n",
    "MAX_STEPS = 1000\n",
    "\n",
    "# Training hyperparameters\n",
    "ENTROPY_BETA = 0.005  # encourages exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f721f",
   "metadata": {},
   "source": [
    "Lets setup the enviorment for the cartpole problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f70ff20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mujoco.MjModel.from_xml_path(MODEL_PATH)\n",
    "data = mujoco.MjData(model)\n",
    "if RENDER_TRAINING:\n",
    "    viewer = mujoco_viewer.MujocoViewer(model, data)\n",
    "else:\n",
    "    viewer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341dbc88",
   "metadata": {},
   "source": [
    "Setting up model and optimizers for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da0fe824",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNetwork()\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "\n",
    "# Running baseline for variance reduction (proper way to normalize)\n",
    "baseline_returns = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d870a9",
   "metadata": {},
   "source": [
    "Now lets Make the training loop for policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3465246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10/300, Avg Reward (last 10): 3.39, Total Reward: -1.67\n",
      "Episode 20/300, Avg Reward (last 10): 2.84, Total Reward: 1.37\n",
      "Episode 20/300, Avg Reward (last 10): 2.84, Total Reward: 1.37\n",
      "Episode 30/300, Avg Reward (last 10): 4.95, Total Reward: 17.61\n",
      "Episode 30/300, Avg Reward (last 10): 4.95, Total Reward: 17.61\n",
      "Episode 40/300, Avg Reward (last 10): 8.33, Total Reward: 21.84\n",
      "Episode 40/300, Avg Reward (last 10): 8.33, Total Reward: 21.84\n",
      "Episode 50/300, Avg Reward (last 10): -1.32, Total Reward: 7.62\n",
      "Episode 50/300, Avg Reward (last 10): -1.32, Total Reward: 7.62\n",
      "Episode 60/300, Avg Reward (last 10): 3.91, Total Reward: -5.43\n",
      "Episode 60/300, Avg Reward (last 10): 3.91, Total Reward: -5.43\n",
      "Episode 70/300, Avg Reward (last 10): 3.01, Total Reward: 9.35\n",
      "Episode 70/300, Avg Reward (last 10): 3.01, Total Reward: 9.35\n",
      "Episode 80/300, Avg Reward (last 10): 3.62, Total Reward: -2.74\n",
      "Episode 80/300, Avg Reward (last 10): 3.62, Total Reward: -2.74\n",
      "Episode 90/300, Avg Reward (last 10): 7.83, Total Reward: 16.71\n",
      "Episode 90/300, Avg Reward (last 10): 7.83, Total Reward: 16.71\n",
      "Episode 100/300, Avg Reward (last 10): 0.91, Total Reward: 1.47\n",
      "Episode 100/300, Avg Reward (last 10): 0.91, Total Reward: 1.47\n",
      "Episode 110/300, Avg Reward (last 10): 4.74, Total Reward: -5.39\n",
      "Episode 110/300, Avg Reward (last 10): 4.74, Total Reward: -5.39\n",
      "Episode 120/300, Avg Reward (last 10): 16.01, Total Reward: 8.50\n",
      "Episode 120/300, Avg Reward (last 10): 16.01, Total Reward: 8.50\n",
      "Episode 130/300, Avg Reward (last 10): -1.52, Total Reward: 29.48\n",
      "Episode 130/300, Avg Reward (last 10): -1.52, Total Reward: 29.48\n",
      "Episode 140/300, Avg Reward (last 10): 6.52, Total Reward: -5.12\n",
      "Episode 140/300, Avg Reward (last 10): 6.52, Total Reward: -5.12\n",
      "Episode 150/300, Avg Reward (last 10): 10.00, Total Reward: -1.57\n",
      "Episode 150/300, Avg Reward (last 10): 10.00, Total Reward: -1.57\n",
      "Episode 160/300, Avg Reward (last 10): -2.31, Total Reward: 3.89\n",
      "Episode 160/300, Avg Reward (last 10): -2.31, Total Reward: 3.89\n",
      "Episode 170/300, Avg Reward (last 10): 20.54, Total Reward: 60.20\n",
      "Episode 170/300, Avg Reward (last 10): 20.54, Total Reward: 60.20\n",
      "Episode 180/300, Avg Reward (last 10): 20.62, Total Reward: 81.45\n",
      "Episode 180/300, Avg Reward (last 10): 20.62, Total Reward: 81.45\n",
      "Episode 190/300, Avg Reward (last 10): 11.96, Total Reward: -20.83\n",
      "Episode 190/300, Avg Reward (last 10): 11.96, Total Reward: -20.83\n",
      "Episode 200/300, Avg Reward (last 10): 9.60, Total Reward: -6.33\n",
      "Episode 200/300, Avg Reward (last 10): 9.60, Total Reward: -6.33\n",
      "Episode 210/300, Avg Reward (last 10): -0.54, Total Reward: -10.73\n",
      "Episode 210/300, Avg Reward (last 10): -0.54, Total Reward: -10.73\n",
      "Episode 220/300, Avg Reward (last 10): -1.59, Total Reward: -1.61\n",
      "Episode 220/300, Avg Reward (last 10): -1.59, Total Reward: -1.61\n",
      "Episode 230/300, Avg Reward (last 10): 5.35, Total Reward: -12.49\n",
      "Episode 230/300, Avg Reward (last 10): 5.35, Total Reward: -12.49\n"
     ]
    }
   ],
   "source": [
    "total_rewards_overtime = []\n",
    "for i in range(EPISODES):\n",
    "    #initialize the environment for each episode to have a random state\n",
    "    data.qpos[0], data.qvel[0], data.qpos[1], data.qvel[1] = Intial_state()\n",
    "    data.ctrl[:] = 0\n",
    "    #Total reward is used to graph\n",
    "    total_reward = 0.0\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    #Lists to store states, actions, and log probabilities for each step\n",
    "    log_probs = []\n",
    "    entropies = []\n",
    "\n",
    "    #Getting states\n",
    "    raw_state = gather_raw_state(data)\n",
    "    state = normalize_state(raw_state)\n",
    "\n",
    "    #Now lets iterate through the steps of the episode\n",
    "    for step in range(MAX_STEPS):\n",
    "\n",
    "        if RENDER_TRAINING:\n",
    "            viewer.render()\n",
    "        \n",
    "        #Getting the action from the policy network\n",
    "        action, log_prob, entropy = sample_action(policy_net, state)\n",
    "        #Storing the log probability and entropy of the action for training later\n",
    "        log_probs.append(log_prob)\n",
    "        entropies.append(entropy)\n",
    "        \n",
    "        if action == 0:\n",
    "            left(data)\n",
    "        else:\n",
    "            right(data)\n",
    "\n",
    "        #Step the simulation forward\n",
    "        mujoco.mj_step(model, data)\n",
    "        next_raw_state = gather_raw_state(data)\n",
    "        next_state = normalize_state(next_raw_state)\n",
    "\n",
    "        #Compute the reward and check if the episode is done\n",
    "        reward = compute_reward(next_raw_state)\n",
    "        \n",
    "        #Accumulate the rewards BEFORE checking done\n",
    "        total_reward += reward\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if episode_finished(next_raw_state):\n",
    "            break\n",
    "\n",
    "        #setting the state to the next state\n",
    "        state = next_state\n",
    "        raw_state = next_raw_state\n",
    "    \n",
    "    # Compute discounted returns (NO normalization within episode!)\n",
    "    returns = compute_discounted_returns(rewards, gamma=0.99)\n",
    "    \n",
    "    # Optional: subtract running baseline for variance reduction\n",
    "    # This is the CORRECT way to use a baseline - compare across episodes, not within\n",
    "    if len(baseline_returns) > 0:\n",
    "        baseline = sum(baseline_returns) / len(baseline_returns)\n",
    "        returns = returns - baseline\n",
    "    \n",
    "    # Update baseline with this episode's returns\n",
    "    baseline_returns.append(returns[0].item())  # Store the initial return\n",
    "    if len(baseline_returns) > 100:  # Keep last 100 episodes\n",
    "        baseline_returns.pop(0)\n",
    "    \n",
    "    # Stack log_probs and entropies into tensors\n",
    "    log_probs_t = torch.stack(log_probs)  # shape [T]\n",
    "    entropies_t = torch.stack(entropies)  # shape [T]\n",
    "    \n",
    "    # Compute policy loss with proper entropy bonus\n",
    "    policy_loss = -(log_probs_t * returns).sum()\n",
    "    entropy_bonus = entropies_t.sum()  # sum of entropies over trajectory\n",
    "    loss = policy_loss - ENTROPY_BETA * entropy_bonus\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    total_rewards_overtime.append(total_reward)\n",
    "    \n",
    "    # Print progress every 10 episodes\n",
    "    if (i + 1) % 10 == 0:\n",
    "        avg_reward = sum(total_rewards_overtime[-10:]) / 10\n",
    "        print(f\"Episode {i+1}/{EPISODES}, Avg Reward (last 10): {avg_reward:.2f}, Total Reward: {total_reward:.2f}\")\n",
    "    \n",
    "    plot_rewards(total_rewards_overtime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
