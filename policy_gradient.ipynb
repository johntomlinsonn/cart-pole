{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7460a5",
   "metadata": {},
   "source": [
    "We will be researching the Cart-pole problem with a policy gradient agorithim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46013b",
   "metadata": {},
   "source": [
    "We Must first build our enviorment that are reinforcement learning model will learn in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a8b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco\n",
    "import mujoco_viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e23484",
   "metadata": {},
   "source": [
    "Now let's create the neccesary functions for our enviorment\n",
    "\n",
    "Our model will take in 4 values: X value, X velocity, Theta value, Theta velocity.\n",
    "\n",
    "We will need the enviorment randomized every new episode played by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a54cb5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rand\n",
    "import math\n",
    "def Intial_state():\n",
    "    theta = rand.uniform(-math.radians(6), math.radians(6))\n",
    "    xpos = rand.uniform(-0.05, 0.05)\n",
    "    xdot = rand.uniform(-0.5, 0.5)\n",
    "    theta_dot = 0\n",
    "    return xpos, xdot, theta, theta_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b5274",
   "metadata": {},
   "source": [
    "Now lets make the neccessary functions for our model. It needs to pushed to the left and to the right. We will use the data.crtl[0] to use the simulated acctuators that are in our model. The data element is all of the data of the cartpole at any moment this includes X Velo, theta Velo, X Pos, Theta Pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "722a33f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "FORCE_MAG = 1.0\n",
    "\n",
    "def right(data, force=FORCE_MAG):\n",
    "    data.ctrl[0] = np.clip(force, -1.0, 1.0)\n",
    "\n",
    "\n",
    "def left(data, force=FORCE_MAG):\n",
    "    data.ctrl[0] = np.clip(-force, -1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2953de",
   "metadata": {},
   "source": [
    "Our model will need to know the state it is in, so we will create a function to get the raw values of the state for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99df24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_raw_state(data):\n",
    "    xpos = data.qpos[0]\n",
    "    xdot = data.qvel[0]\n",
    "    theta = data.qpos[1]\n",
    "    theta_dot = data.qvel[1]\n",
    "    return np.array([xpos, xdot, theta, theta_dot], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee6268",
   "metadata": {},
   "source": [
    "Now we will create the reward funcition for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02579409",
   "metadata": {},
   "outputs": [],
   "source": [
    "THETA_LIMIT_RADIANS = math.radians(12)\n",
    "X_LIMIT = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e0fd652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(raw_state):\n",
    "    x, x_dot, theta, theta_dot = raw_state\n",
    "    angle_error = abs(theta)\n",
    "    position_error = abs(x)\n",
    "    reward = 1.0\n",
    "    reward -= 2.0 * (angle_error / THETA_LIMIT_RADIANS)\n",
    "    reward -= 0.5 * (position_error / X_LIMIT)\n",
    "    reward -= 0.01 * (abs(x_dot) + abs(theta_dot))\n",
    "    return max(reward, -2.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d087c1",
   "metadata": {},
   "source": [
    "Now we will make a function to check if an episode is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a10ab3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_finished(raw_state):\n",
    "    x, _, theta, _ = raw_state\n",
    "    return abs(theta) > THETA_LIMIT_RADIANS or abs(x) > X_LIMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934afc0a",
   "metadata": {},
   "source": [
    "Now we must create our Nueral Network for the cartpole problem\n",
    "We will be creating a nueral netowrk through pytorch. It will take in 4 dimensions as an input for the network: X Value, X Velocity, Theta value, Theta Velocity\n",
    "\n",
    "We will have 3 layers in the netowrk, teh first will take in the state layer, second being an input from the frist, and the thrid being an input from the second and outputting an action from our action set. Our action set is either sliding to the left or sliding to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e60ebeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self,state_dim = 4, action_dim = 2, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.softmax(self.layer2(x), dim=-1)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f717110",
   "metadata": {},
   "source": [
    "We will also make a sample action functino for the nueral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2441426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(policy_net, state):\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        action_probs = policy_net(state)\n",
    "    action_distribution = torch.distributions.Categorical(logits=action_probs)\n",
    "    action = action_distribution.sample()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fbe5c1",
   "metadata": {},
   "source": [
    "Now lets add our necessary constants for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "687f9f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_TRAINING = True\n",
    "MODEL_PATH = 'cartpole.xml'\n",
    "EPISODES = 300\n",
    "MAX_STEPS = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f721f",
   "metadata": {},
   "source": [
    "Lets setup the enviorment for the cartpole problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f70ff20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mujoco.MjModel.from_xml_path(MODEL_PATH)\n",
    "data = mujoco.MjData(model)\n",
    "if RENDER_TRAINING:\n",
    "    viewer = mujoco_viewer.MujocoViewer(model, data)\n",
    "else:\n",
    "    viewer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d870a9",
   "metadata": {},
   "source": [
    "Now lets Make the training loop for policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3465246",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m     viewer.render()\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#Getting the action from the policy network\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m action = sample_action(\u001b[43mpolicy_net\u001b[49m, state)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m#Storing the log probability of the action for training later\u001b[39;00m\n\u001b[32m     21\u001b[39m log_probs.append(action)\n",
      "\u001b[31mNameError\u001b[39m: name 'policy_net' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(EPISODES):\n",
    "    #initialize the environment for each episode to have a random state\n",
    "    data.qpos[0], data.qvel[0], data.qpos[1], data.qvel[1] = Intial_state()\n",
    "    data.ctrl[:] = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    #Lists to store states, actions, and log probabilities for each step\n",
    "    log_probs = []\n",
    "\n",
    "\n",
    "    #Now lets iterate through the steps of the episode\n",
    "    for i in range(MAX_STEPS):\n",
    "        state = gather_raw_state(data)\n",
    "\n",
    "        if RENDER_TRAINING:\n",
    "            viewer.render()\n",
    "        \n",
    "        #Getting the action from the policy network\n",
    "        action = sample_action(policy_net, state)\n",
    "        #Storing the log probability of the action for training later\n",
    "        log_probs.append(action)\n",
    "        if action.item() == 0:\n",
    "            left(data)\n",
    "        else:\n",
    "            right(data)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
